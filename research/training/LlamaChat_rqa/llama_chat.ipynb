{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose model id \n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "quantization_config =transformers.BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "#load model\n",
    "llama_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\":3},\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "llama_model.eval()\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "#load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline to generate text, set model parameters\n",
    "generate_text = transformers.pipeline(\n",
    "    model=llama_model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  \n",
    "    task='text-generation',\n",
    "    temperature=0.75,  \n",
    "    max_new_tokens=300,  \n",
    "    repetition_penalty=1.1  \n",
    "\n",
    "#create hf pipeline -> needed for langchain \n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "model_norm = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create prompt for qa in LLama-2 prompt style\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\" \n",
    "def get_prompt(instruction, system_prompt ):\n",
    "        SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
    "        prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "        return prompt_template\n",
    "sys_prompt = \"\"\"\\\n",
    "You are a helpful, plant caretaking assistant. Use the following pieces of information to answer the user's question. You can assume that the information is always about the plant in question even if the name is different.\n",
    "If you don't know the answer, just say that you don't know, do under no circumstances try to make up an answer.\"\"\"\n",
    "\n",
    "instruction = \"\"\"\n",
    "CONTEXT:/n/n {context}/n\n",
    "\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_answer(question, file):\n",
    "    \"\"\"\n",
    "    This function takes the question/user input as well as the filepath of the text file\n",
    "    belonging to the plant in question as inputs and generates an answer with langchain\n",
    "    retrieval question answering\n",
    "    \"\"\"\n",
    "    loader = TextLoader(file)\n",
    "    documents = loader.load()\n",
    "\n",
    "    #embed document\n",
    "    embedding = model_norm\n",
    "    vectordb = Chroma.from_documents(documents=documents,\n",
    "                                    embedding=embedding)\n",
    "    \n",
    "    #initialize retriever, in this case only on document to choose\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "    #create prompt\n",
    "    prompt_template = get_prompt(instruction, sys_prompt)\n",
    "\n",
    "    llama_prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    chain_type_kwargs = {\"prompt\": llama_prompt}\n",
    "\n",
    "    #initialize retrieval augemented generation pipeline\n",
    "    rag_pipeline = RetrievalQA.from_chain_type(\n",
    "        llm=llm, chain_type='stuff',\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs=chain_type_kwargs,)\n",
    "\n",
    "    return(rag_pipeline(question))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
