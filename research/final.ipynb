{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randrange\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "class LMTrainingPipeline:\n",
    "    def __init__(self, model_id, dataset_path, output_dir):\n",
    "        self.model_id = model_id\n",
    "        self.dataset_path = dataset_path\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def install_dependencies(self):\n",
    "        # Install necessary dependencies\n",
    "        #pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade\n",
    "        pass\n",
    "    \n",
    "    def load_and_preprocess_data(self):\n",
    "        # Load and preprocess dataset\n",
    "        dataset = load_dataset(\"csv\", data_files=self.dataset_path)[\"train\"]\n",
    "        my_dict = dataset[:32]\n",
    "        self.dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "    def format_instruction(self, sample):\n",
    "        return f\"\"\"### Instruction:\n",
    "    Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{sample['title']}\n",
    "\n",
    "### Response:\n",
    "{sample['comment']}\n",
    "\"\"\"\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        # Load tokenizer and model\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "        self.model.config.pretraining_tp = 1\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "    def prepare_model_for_training(self):\n",
    "        # Define LoRA configuration\n",
    "        self.peft_config = LoraConfig(\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            r=64,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        # Prepare model for training\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        self.model = get_peft_model(self.model, self.peft_config)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Define training arguments\n",
    "        self.args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=2,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            tf32=False,\n",
    "            max_grad_norm=0.3,\n",
    "            warmup_ratio=0.03,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the trainer\n",
    "        max_seq_length = 2048\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.dataset,\n",
    "            peft_config=self.peft_config,\n",
    "            max_seq_length=max_seq_length,\n",
    "            tokenizer=self.tokenizer,\n",
    "            packing=True,\n",
    "            formatting_func=self.format_instruction,\n",
    "            args=self.args,\n",
    "        )\n",
    "\n",
    "        # # Train the model\n",
    "        # trainer.train()\n",
    "\n",
    "        # # Save the trained model\n",
    "        # trainer.save_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Optional: Unpatch flash attention if used\n",
    "        if use_flash_attention:\n",
    "            from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "            unplace_flash_attn_with_attn()\n",
    "\n",
    "        # Load trained model and tokenizer\n",
    "        self.args.output_dir = self.output_dir\n",
    "        self.learned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            self.args.output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        self.learned_model_tokenizer = AutoTokenizer.from_pretrained(self.args.output_dir)\n",
    "    \n",
    "    def generate_answer(self, question)\n",
    "        # Create a prompt for model generation\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "        # Generate instructions using the model\n",
    "        input_ids = self.learned_model_tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "        outputs = self.learned_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.9)\n",
    "\n",
    "        # Display results\n",
    "        print(f\"Prompt:\\n{sample['title']}\\n\")\n",
    "        print(f\"Generated instruction:\\n{self.tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "        print(f\"Ground truth:\\n{sample['comment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flash_attention = False\n",
    "\n",
    "# Initialize and execute the training pipeline\n",
    "pipeline = LMTrainingPipeline(\n",
    "    model_id=\"NousResearch/Llama-2-7b-hf\",  # Update with your desired model\n",
    "    dataset_path=\"PlantQA.csv\",  # Update with your dataset path\n",
    "    output_dir=\"llama-7-int4-plantqa/checkpoint-5088\",  # Update with your desired output directory\n",
    ")\n",
    "\n",
    "pipeline.install_dependencies()\n",
    "pipeline.load_and_preprocess_data()\n",
    "pipeline.load_model_and_tokenizer()\n",
    "pipeline.prepare_model_for_training()\n",
    "pipeline.train_model()\n",
    "pipeline.load_model()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
