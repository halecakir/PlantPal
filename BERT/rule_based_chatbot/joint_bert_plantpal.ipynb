{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "from seqeval.metrics import classification_report\n",
    "import shutil\n",
    "import pickle\n",
    "from seqeval.metrics import classification_report\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-base-uncased'\n",
    "MAX_SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointIntentAndSlotFillingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, total_intent_no=None, total_slot_no=None,\n",
    "                 model_name=BERT_MODEL, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name)\n",
    "        self.dropout = Dropout(dropout_prob)\n",
    "        self.intent_classifier = Dense(total_intent_no, activation='softmax')\n",
    "        self.slot_classifier = Dense(total_slot_no, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        bert_output = self.bert(inputs)\n",
    "\n",
    "        sequence_output = self.dropout(bert_output[0])\n",
    "        slots_predicted = self.slot_classifier(sequence_output)\n",
    "\n",
    "        pooled_output = self.dropout(bert_output[1])\n",
    "        intent_predicted = self.intent_classifier(pooled_output)\n",
    "\n",
    "        return slots_predicted, intent_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/home/CE/lkiefer/BERT/rule_based_chatbot/joint_model/intent_label_encoder.pkl','rb') as fp:\n",
    "  intent_le = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/home/CE/lkiefer/BERT/rule_based_chatbot/joint_model/seq_out_index_word.pkl', 'rb') as fp:\n",
    "  index_to_word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CE/lkiefer/BERT/rule_based_chatbot/checkpoints/joint_model_weights_0.49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 16:10:56.411295: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL = 'bert-base-uncased'\n",
    "#tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "#load model checkpoints\n",
    "checkpoint_dirs = r'/home/CE/lkiefer/BERT/rule_based_chatbot/checkpoints'\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dirs)\n",
    "#latest = r'/home/CE/lkiefer/BERT/rule_based_chatbot/checkpoints/joint_model_weights_0.49.ckpt.data-00000-of-00001'\n",
    "print(latest)\n",
    "\n",
    "model = JointIntentAndSlotFillingModel(\n",
    "    total_intent_no=3, total_slot_no=17,dropout_prob=0.1)\n",
    "\n",
    "#model.built = True\n",
    "\n",
    "model.load_weights(latest)\n",
    "\n",
    "opt = Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "losses = [SparseCategoricalCrossentropy(from_logits=False),\n",
    "          SparseCategoricalCrossentropy(from_logits=False)]\n",
    "metrics = [SparseCategoricalAccuracy('accuracy')]\n",
    "model.compile(optimizer=opt, loss=losses, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(slots):\n",
    "\n",
    "  result = {}\n",
    "  current_key = ''\n",
    "  current_value = None\n",
    "\n",
    "  for key, value in slots.items():\n",
    "      if key.startswith('##'):\n",
    "          current_key += key[2:]\n",
    "      else:\n",
    "          if current_key:\n",
    "              result[current_key] = current_value\n",
    "          current_key = key\n",
    "          current_value = value\n",
    "\n",
    "  if current_key:\n",
    "      result[current_key] = current_value\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(text):\n",
    "\n",
    "  tokenized_sent = tokenizer.encode(text)\n",
    "\n",
    "  predicted_slots, predicted_intents = model.predict([tokenized_sent])\n",
    "\n",
    "  intent = intent_le.inverse_transform([np.argmax(predicted_intents)])\n",
    "  #print(\"=\"*5, \"INTENT\", \"=\"*5)\n",
    "  #print(intent)\n",
    "\n",
    "  slots = np.argmax(predicted_slots, axis=-1)\n",
    "\n",
    "  slots = [index_to_word[w_idx] for w_idx in slots[0]]\n",
    "\n",
    "  #print(\"\\n\")\n",
    "  #print(\"=\"*5, \"SLOTS\",\"=\"*5)\n",
    "\n",
    "  slot_dict = {}\n",
    "\n",
    "  for w,l in zip(tokenizer.tokenize(text),slots[1:-1]):\n",
    "    #print(w,  l)\n",
    "    slot_dict[w] = l\n",
    "\n",
    "  slot_dict = detokenize(slot_dict)\n",
    "\n",
    "  return intent, slot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_question():\n",
    "    user = input(\"Please ask a different question.\")\n",
    "    return user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def full_slots(slot_dict):   \n",
    "    full_slots = {}\n",
    "    current_key = None\n",
    "    current_value = None\n",
    "\n",
    "    for key, value in slot_dict.items():\n",
    "        if value.startswith('B-'):\n",
    "            if current_key:\n",
    "                full_slots[current_key] = current_value\n",
    "            current_key = key\n",
    "            current_value = value[2:]  # Remove the 'B-' prefix\n",
    "        elif value.startswith('I-'):\n",
    "            if current_key:\n",
    "                current_key += ' ' + key\n",
    "                current_value = value[2:]  # Remove the 'I-' prefix\n",
    "        else:\n",
    "            if current_key:\n",
    "                full_slots[current_key] = current_value\n",
    "                current_key = None\n",
    "                current_value = None\n",
    "            full_slots[key] = value\n",
    "\n",
    "    # Handle the last entry\n",
    "    if current_key:\n",
    "        full_slots[current_key] = current_value\n",
    "\n",
    "    return full_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 'O',\n",
       " 'need help': 'help',\n",
       " 'taking': 'O',\n",
       " 'care': 'O',\n",
       " 'of': 'O',\n",
       " 'my': 'O',\n",
       " 'monstera plant': 'PlantName',\n",
       " '.': 'PAD'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {'i': 'O', 'need': 'B-help', 'help': 'I-help', 'taking': 'O', 'care': 'O', 'of': 'O', 'my': 'O', 'monstera': 'B-PlantName', 'plant': 'I-PlantName', '.': 'PAD'}\n",
    "\n",
    "full_slots(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(user, intent, slot_dict):\n",
    "    print(slot_dict)\n",
    "    if intent != 'plant_caretaking_advice':\n",
    "        user = diff_question()\n",
    "        intent, slots = show_predictions(user)\n",
    "        conversation(user, intent, slots)\n",
    "    else:\n",
    "        slots = full_slots(slot_dict)\n",
    "        plant_name = None\n",
    "        for k,v in slots.items():\n",
    "            if v == \"PlantName\" in slots.values():\n",
    "                plant_name = k\n",
    "            \n",
    "    if plant_name == None:\n",
    "        plant_name = input(\"Please state the name of your plant!\")\n",
    "        \n",
    "    print(\"Plant name: \", plant_name)\n",
    "    print(\"slots\", slots)\n",
    "    return slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maintenance: \n",
      "size: Diameter â‰¥ 20 cm, height 20-30 cm\n",
      "soil: Peat or soil with specific nutrients\n",
      "sunlight: Like half shade, may grow in places with bright scattered light\n",
      "watering: Water thoroughly when soil is dry\n",
      "fertilization: Mix proper amount of slow-release fertilizers when planting, apply liquid fertilizers 1-2 times monthly\n",
      "pruning: Remove dead and diseased branches timely\n"
     ]
    }
   ],
   "source": [
    "def search_plant(folder_path, search_string):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\") and search_string == filename:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r') as json_file:\n",
    "                content = json.load(json_file)\n",
    "                maintenance_info = content[\"maintenance\"]\n",
    "                print(\"maintenance: \")\n",
    "                for key, value in maintenance_info.items():\n",
    "                    print(f\"{key}: {value}\")\n",
    "                #print(f\"Content of {filename}:\")\n",
    "                #print(json.dumps(content, indent=4))\n",
    "                #print(\"-\" * 40)\n",
    "\n",
    "# Replace these with your desired folder path and search string\n",
    "folder_path = \"/home/CE/lkiefer/BERT/data/plant-database-master/json\"\n",
    "search_string = \"actaea racemosa.json\"\n",
    "\n",
    "search_plant(folder_path, search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n",
      "{'i': 'O', 'need': 'O', 'help': 'O', 'taking': 'O', 'care': 'I-AdviceType', 'of': 'O', 'my': 'O', 'abelia': 'O', '.': 'PAD'}\n",
      "Plant name:  abelia.\n",
      "slots {'i': 'O', 'need': 'O', 'help': 'O', 'taking': 'O', 'of': 'O', 'my': 'O', 'abelia': 'O', '.': 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "user = input(\"Hi! I am PlantPal. How can I help you?\")\n",
    "intent, slots = show_predictions(user)\n",
    "all_slots = conversation(user, intent, slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 'O', 'need': 'O', 'help': 'O', 'taking': 'O', 'care': 'O', 'of': 'O', 'my': 'O', 'pothos plant': 'PlantName', '.': 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "print(all_slots)\n",
    "# I need help taking care of my pothos plant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
